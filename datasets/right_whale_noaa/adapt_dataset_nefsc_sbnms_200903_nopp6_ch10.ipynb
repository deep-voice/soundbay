{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48836b17-90bd-4aaf-a3eb-9fe74abd485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from soundbay.utils.metadata_processing import (\n",
    "    bg_from_non_overlap_calls,\n",
    "    correct_call_times_with_duration,\n",
    "    non_overlap_df,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4c6ea5",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d895fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "\n",
    "deepvoice_directory = os.path.dirname(\n",
    "    os.path.dirname(os.path.dirname(current_directory))\n",
    ")\n",
    "\n",
    "nefsc_folder_path = os.path.join(\n",
    "    deepvoice_directory, \"datasets\", \"nefsc_sbnms_200903_nopp6_ch10\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b78b9b3-727e-4ad6-9c36-c5d57050d2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f\"{nefsc_folder_path}/detections/NEFSC_SBNMS_200903_NOPP6_CH10_allbaleen_detection_log.csv\"\n",
    "original_metadata = pd.read_csv(data_path)\n",
    "audio_files_path = f\"{nefsc_folder_path}/source-audio/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e7ae48",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac9a25b6-83e9-41ea-b278-702b1c8fa5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILES_DELTA = 15 * 60  # 15 minutes files\n",
    "\n",
    "\n",
    "def get_sec(time_str):\n",
    "    \"\"\"Get seconds from time.\"\"\"\n",
    "    h, m, s = time_str.split(\":\")\n",
    "    return int(h) * 3600 + int(m) * 60 + int(s)\n",
    "\n",
    "\n",
    "def filetime_from_time(time_str):\n",
    "    sec_time = get_sec(time_str)\n",
    "    filetime_sec = math.floor(sec_time / FILES_DELTA) * FILES_DELTA\n",
    "    filetime_str = time.strftime(\"%H%M%S\", time.gmtime(filetime_sec))\n",
    "    return filetime_str\n",
    "\n",
    "\n",
    "def get_time_and_date(iso_input):\n",
    "    date, time_str = iso_input.split(\"T\")\n",
    "    time_str = time_str.split(\"-\")[0]\n",
    "    return date, time_str\n",
    "\n",
    "\n",
    "def iso_to_file_name(iso_input):\n",
    "    date, time_str = get_time_and_date(iso_input)\n",
    "    filename_time = filetime_from_time(time_str)\n",
    "    filename_date = date.replace(\"-\", \"\")\n",
    "    return f\"NOPP6_EST_{filename_date}_{filename_time}_CH10\"\n",
    "\n",
    "\n",
    "def get_time_in_file(iso_input, type=\"start\"):\n",
    "    date, time_str = get_time_and_date(iso_input)\n",
    "    time_int = get_sec(time_str)\n",
    "    time_in_file = time_int - (time_int // FILES_DELTA) * FILES_DELTA\n",
    "    if type == \"end\" and time_in_file == 0:\n",
    "        time_in_file = FILES_DELTA\n",
    "    return time_in_file\n",
    "\n",
    "\n",
    "def get_previous_filename(filename):\n",
    "    file_parts = filename.split(\"_\")\n",
    "    time_part = file_parts[-2]\n",
    "    h, m, s = re.findall(\"..\", time_part)\n",
    "    time_sec = int(h) * 3600 + int(m) * 60 + int(s)\n",
    "    new_time = time_sec - FILES_DELTA\n",
    "    assert new_time % FILES_DELTA == 0, \"whyyyy\"\n",
    "    new_time_part = time.strftime(\"%H%M%S\", time.gmtime(new_time))\n",
    "    file_parts[-2] = new_time_part\n",
    "    return \"_\".join(file_parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ffee4d",
   "metadata": {},
   "source": [
    "## Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fbd8273-7d9c-4515-9e39-1bc50be63f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_metadata[\"filename\"] = [\n",
    "    iso_to_file_name(x) for x in original_metadata[\"End_DateTime_ISO8601\"]\n",
    "]\n",
    "original_metadata[\"begin_time\"] = [\n",
    "    get_time_in_file(x) for x in original_metadata[\"Start_DateTime_ISO8601\"]\n",
    "]\n",
    "original_metadata[\"end_time\"] = [\n",
    "    get_time_in_file(x, \"end\")\n",
    "    for x in original_metadata[\"End_DateTime_ISO8601\"]\n",
    "]\n",
    "original_metadata[\"call_length\"] = (\n",
    "    original_metadata[\"end_time\"] - original_metadata[\"begin_time\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ee992",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_metadata['Species'].value_counts())\n",
    "original_metadata['Species'] = original_metadata['Species'].apply(lambda x: 'HUWH' if 'HUWH' in x else x)\n",
    "print(original_metadata['Species'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7260d445-37d9-4d59-8b88-37637d1e50e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split annotations that originate from different files to the corresponding files\n",
    "problematic_samples_filter = original_metadata[\"call_length\"] < 0\n",
    "after_split_samples = original_metadata[problematic_samples_filter].copy()\n",
    "original_metadata.loc[problematic_samples_filter, \"end_time\"] = FILES_DELTA\n",
    "original_metadata.loc[problematic_samples_filter, \"filename\"] = [\n",
    "    get_previous_filename(x)\n",
    "    for x in original_metadata[problematic_samples_filter][\"filename\"]\n",
    "]\n",
    "after_split_samples[\"begin_time\"] = 0\n",
    "new_metadata = pd.concat(\n",
    "    [original_metadata, after_split_samples], ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e586e55f-22a6-4c4f-bb28-d34c8e1fd8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove calls with length of zero\n",
    "new_metadata = new_metadata[new_metadata[\"call_length\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa4efcfa-19aa-4ac0-8be8-9342fdbd39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct files duration\n",
    "new_metadata = correct_call_times_with_duration(\n",
    "    new_metadata, audio_files_path=audio_files_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26da75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metadata[\"Detection_Confidence\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05480269",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01ba5659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by species and process each separately\n",
    "species_dfs = []\n",
    "for species in new_metadata['Species'].unique():\n",
    "    species_df = new_metadata[new_metadata['Species'] == species]\n",
    "    species_df = non_overlap_df(species_df)\n",
    "    species_dfs.append(species_df)\n",
    "\n",
    "# Combine back into single dataframe\n",
    "new_metadata = pd.concat(species_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c036cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metadata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f656a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metadata[\"Detection_Confidence\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c720729c-7c8f-4f62-b61d-09f5a486dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge overlapping calls\n",
    "new_metadata = non_overlap_df(new_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7338da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_metadata[\"unique_id\"] = (\n",
    "    new_metadata.index.astype(str)\n",
    "    + new_metadata[\"Selection\"].astype(str)\n",
    "    + new_metadata[\"filename\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afc60dd6-efd9-4a92-ba05-47cd5397bd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_bg_metadata_all = bg_from_non_overlap_calls(new_metadata)\n",
    "with_bg_metadata_all = with_bg_metadata_all.sort_values(\n",
    "    by=[\"filename\", \"begin_time\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73b5fe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping from species to unique labels\n",
    "species_to_label_all = {\n",
    "    species: idx + 1\n",
    "    for idx, species in enumerate(with_bg_metadata_all[\"Species\"].unique())\n",
    "}\n",
    "\n",
    "# Assign these labels to the 'label' column where it is non-zero\n",
    "with_bg_metadata_all.loc[with_bg_metadata_all[\"label\"] != 0, \"label\"] = (\n",
    "    with_bg_metadata_all[\"Species\"].map(species_to_label_all)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1eca64be-1f95-4090-a33a-f45c024b4b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_bg_metadata_all[\"label\"] = np.array(with_bg_metadata_all[\"label\"]).astype(\n",
    "    \"int\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09ad31e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_bg_metadata_all = with_bg_metadata_all[with_bg_metadata_all[\"Detection_Confidence\"] == \"Detected\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f768b9d-aa4a-416c-b254-b272b4af41a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to train and val\n",
    "val_filter = with_bg_metadata_all[\"filename\"].str.contains(\"20090330\")\n",
    "train_metadata = with_bg_metadata_all[~val_filter]\n",
    "val_metadata = with_bg_metadata_all[val_filter]\n",
    "print(f\"{len(train_metadata)=}, {len(val_metadata)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e61ab0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the common species between train and validation datasets\n",
    "common_species = set(train_metadata['Species']).intersection(set(val_metadata['Species']))\n",
    "\n",
    "# Filter the dataframes to keep only the common species\n",
    "train_metadata = train_metadata[train_metadata['Species'].isin(common_species)]\n",
    "val_metadata = val_metadata[val_metadata['Species'].isin(common_species)]\n",
    "\n",
    "# Check the number of unique species again\n",
    "assert train_metadata['Species'].nunique() == val_metadata['Species'].nunique(), \"Species counts still do not match!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db528385",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_label_dict = dict(zip(train_metadata[train_metadata.label!=0]['Species'], train_metadata[train_metadata.label!=0]['label']))\n",
    "species_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629e89e0-86cc-4b40-9163-99017bef3d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_in_hrs_format(time_secs):\n",
    "    return (\n",
    "        time_secs // 3600,\n",
    "        (time_secs - time_secs // 3600 * 3600) // 60,\n",
    "        time_secs % 60,\n",
    "    )\n",
    "\n",
    "\n",
    "for name, meta in {\n",
    "    \"train\": train_metadata,\n",
    "    \"val\": val_metadata,\n",
    "    \"with_bg_metadata_all\": with_bg_metadata_all,\n",
    "}.items():\n",
    "    print(name)\n",
    "    print(f\"Number of samples: {len(meta)}\")\n",
    "    print(f\"Labels breakdown: {meta['label'].value_counts()}\")\n",
    "    h, m, s = get_time_in_hrs_format(\n",
    "        meta[\"call_length\"][meta[\"label\"] == 1].sum()\n",
    "    )\n",
    "    print(f\"Calls length: {h}:{m}:{s}\")\n",
    "    h, m, s = get_time_in_hrs_format(\n",
    "        meta[\"call_length\"][meta[\"label\"] == 0].sum()\n",
    "    )\n",
    "    print(f\"Background length: {h}:{m}:{s}\")\n",
    "    print(\n",
    "        \"-----------------------------------------------------------------------------\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b73bf6fe-ac3e-4d5c-8156-5548f2d24f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata.to_csv(\"train.csv\", index=False)\n",
    "val_metadata.to_csv(\"val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f91f384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a31b542",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata = pd.read_csv(\"train.csv\")\n",
    "val_metadata = pd.read_csv(\"val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aaf57a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "huwh_train = train_metadata.copy()\n",
    "huwh_train.loc[huwh_train.label != species_label_dict[\"HUWH\"], \"label\"] = 0\n",
    "huwh_train.loc[huwh_train.label == species_label_dict[\"HUWH\"], \"label\"] = 1\n",
    "\n",
    "riwh_train = train_metadata.copy()\n",
    "riwh_train.loc[riwh_train.label != species_label_dict[\"RIWH\"], \"label\"] = 0\n",
    "riwh_train.loc[riwh_train.label == species_label_dict[\"RIWH\"], \"label\"] = 1\n",
    "\n",
    "huwh_train.to_csv(\"train_huwh.csv\", index=False)\n",
    "riwh_train.to_csv(\"train_riwh_nefsc_sbnms_200903_nopp6_ch10.csv\", index=False)\n",
    "\n",
    "huwh_val = val_metadata.copy()\n",
    "huwh_val.loc[huwh_val.label != species_label_dict[\"HUWH\"], \"label\"] = 0\n",
    "huwh_val.loc[huwh_val.label == species_label_dict[\"HUWH\"], \"label\"] = 1\n",
    "\n",
    "riwh_val = val_metadata.copy()\n",
    "riwh_val.loc[riwh_val.label != species_label_dict[\"RIWH\"], \"label\"] = 0\n",
    "riwh_val.loc[riwh_val.label == species_label_dict[\"RIWH\"], \"label\"] = 1\n",
    "\n",
    "huwh_val.to_csv(\"val_huwh.csv\", index=False)\n",
    "riwh_val.to_csv(\"val_riwh_nefsc_sbnms_200903_nopp6_ch10.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a49e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24896347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c651a7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soundbay",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
