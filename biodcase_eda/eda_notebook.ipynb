{
 "cells": [
  {
   "cell_type": "code",
   "id": "cfe27b9a-dcac-476f-8f8b-646476469a71",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from biodcase_eda.helpers import load_csvs, distribution_per_source\n",
    "from soundbay.utils.metadata_processing import correct_call_times_with_duration, bg_from_non_overlap_calls, non_overlap_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fbd14967-9344-4a40-9b33-cfb89af342eb",
   "metadata": {},
   "source": [
    "base_location = \"/Users/shai/personal/deepvoice\"\n",
    "dataset_path = Path(base_location) / 'biodcase' / 'datasets'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8aeceea1",
   "metadata": {},
   "source": [
    "# Default audio preprocessing parameters from config\n",
    "SAMPLE_RATE = 250  # Default sample rate for processing\n",
    "DATA_SAMPLE_RATE = 250  # Original sample rate of the audio files\n",
    "N_FFT = 128  # Number of FFT points\n",
    "HOP_LENGTH = 16  # Hop length between windows\n",
    "N_MELS = 64  # Number of mel frequency bins\n",
    "F_MIN = 0  # Minimum frequency\n",
    "F_MAX = 125  # Maximum frequency\n",
    "\n",
    "# Dataset parameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 4\n",
    "SEQ_LENGTH = 10  # Length of each segment in seconds\n",
    "MARGIN_RATIO = 0.25  # Ratio of margin around the annotation\n",
    "SLICE_FLAG = False  # Whether to slice the audio into smaller segments\n",
    "LABEL_TYPE = 'single_label'\n",
    "PATH_HIERARCHY = 1\n",
    "\n",
    "NUMERIC_MAPPING = {'bma': 1, 'bmb': 2, 'bmd': 3, 'bmz': 4, 'bp20': 5, 'bp20plus': 6, 'bpd': 7}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a4ee5c0a",
   "metadata": {},
   "source": [
    "def process_annotations_csvs(annotations_dir: Path, split: str) -> pd.DataFrame:\n",
    "    all_dfs = load_csvs(annotations_dir, split)\n",
    "    # Combine all DataFrames\n",
    "    result_df = pd.concat(all_dfs, ignore_index=True).assign(label=lambda x: x['annotation'].map(NUMERIC_MAPPING), filename=lambda x: x.dataset + \"/\" + x.filename.str.replace(\".wav\", \"\"))\n",
    "    \n",
    "    bg_df = bg_from_non_overlap_calls(non_overlap_df(result_df.assign(label=1)).assign(label= 1))\n",
    "\n",
    "    # concat calls and background\n",
    "    df = pd.concat([\n",
    "        result_df, bg_df.query('label == 0')]\n",
    "        ).sort_values('filename').reset_index()\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_files_length(df: pd.DataFrame, annotations_dir: Path) -> pd.DataFrame:\n",
    "    # there are 5 files with wrong annotations, e.g. :\n",
    "    #       dataset\t            filename\t\t\t                            start_datetime\t\n",
    "    # 10293\telephantisland2014\telephantisland2014/2014-10-05T02-00-00_000\t\t2014-10-05 03:01:37.027000+00:00\n",
    "    return correct_call_times_with_duration(df, audio_files_path=str(annotations_dir.parent / \"audio\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b597f155",
   "metadata": {},
   "source": [
    "df_train = process_annotations_csvs(dataset_path / \"train\" / \"annotations\", 'train')\n",
    "df_val = process_annotations_csvs(dataset_path / \"validation\" / \"annotations\", 'validation')\n",
    "df = pd.concat([df_train, df_val])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "791c08b3",
   "metadata": {},
   "source": [
    "df.sort_values(['filename', 'begin_time']).head(5)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7faba94a",
   "metadata": {},
   "source": "distribution_per_source(df)\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e2be8e13",
   "metadata": {},
   "source": [
    "\n",
    "def plot_distribution(df, group_by, title, figsize=(20, 5)):\n",
    "    \"\"\"Plot distribution of call lengths for a given grouping.\"\"\"\n",
    "    # Set style\n",
    "    plt.style.use('seaborn')\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create figure\n",
    "    n_groups = len(df[group_by].unique())\n",
    "    n_cols = min(n_groups, 4)  # Max 4 columns\n",
    "    n_rows = (n_groups + n_cols - 1) // n_cols  # Ceiling division\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    fig.suptitle(title, fontsize=16, y=1.02)\n",
    "    \n",
    "    # Flatten axes if needed\n",
    "    if n_rows > 1:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    # Plot histograms\n",
    "    for i, (group, data) in enumerate(df.groupby(group_by)):\n",
    "        sns.histplot(data=data['call_length'], \n",
    "                    ax=axes[i],\n",
    "                    bins=30,\n",
    "                    kde=True)\n",
    "        axes[i].set_title(group)\n",
    "        axes[i].set_xlabel('Call Length (seconds)')\n",
    "        axes[i].set_ylabel('Count')\n",
    "        axes[i].set_xlim(0, data['call_length'].max() * 1.1)\n",
    "    \n",
    "    # Remove empty subplots\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        fig.delaxes(axes[j])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot by dataset\n",
    "plot_distribution(df, 'dataset', 'Call Length Distribution by Dataset', figsize=(20, 10))\n",
    "# Plot by annotation\n",
    "plot_distribution(df, 'annotation', 'Call Length Distribution by Annotation', figsize=(20, 5))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Train and Val analysis\n",
    "per dataset frequency analysis + train Vs val analysis"
   ],
   "id": "b99b2fc399af2d3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_df_only_calls = df.loc[(df['source'] == \"train\") & (df['label'] != 0)]\n",
    "val_df_only_calls = df.loc[(df['source'] == \"validation\") & (df['label'] != 0)]"
   ],
   "id": "e1fb3eec376cd122",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from biodcase_eda.helpers import boxplot_freq_per_label, segment_frequency_overlap\n",
    "def analysis_per_source(df_: pd.DataFrame, source: str) -> None:\n",
    "    print('\\n\\n SOURCE:', source)\n",
    "    boxplot_freq_per_label(df_)\n",
    "    segments = segment_frequency_overlap(df_)\n",
    "    print(segments)\n",
    "analysis_per_source(train_df_only_calls, 'train')\n",
    "analysis_per_source(val_df_only_calls, 'validation')"
   ],
   "id": "a98f1897e0604817",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "## train vs val\n",
    "from biodcase_eda.helpers import compare_dataset_sources, barplot_labels_per_dataset, plot_frequency_distributions, plot_call_length\n",
    "def compare_train_val(train_df, val_df):\n",
    "    compare_dataset_sources(train_df, val_df)\n",
    "    barplot_labels_per_dataset(train_df, val_df)\n",
    "    plot_frequency_distributions(train_df, val_df)\n",
    "    plot_call_length(train_df, val_df)\n",
    "\n",
    "compare_train_val(train_df_only_calls, val_df_only_calls)"
   ],
   "id": "55da433b61079594",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## save the final csv",
   "id": "baf648641d1083b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for s in [\"train\", \"validation\"]:\n",
    "    print('SOURCE:', s)\n",
    "    df[df['source'] == s].to_csv(dataset_path / s / 'all_annotations.csv', index=False)"
   ],
   "id": "cf8874d011c9da3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Assuming we saved the final csv as \"all_annotations.csv\" - Load wavs and see some spectrograms",
   "id": "153a1f6fb5e818db"
  },
  {
   "cell_type": "code",
   "id": "6b84a686",
   "metadata": {},
   "source": [
    "from soundbay.data import ClassifierDataset\n",
    "\n",
    "# Create dataset with minimal required parameters\n",
    "dataset = ClassifierDataset(\n",
    "    data_path=str(dataset_path / \"train/audio\"),\n",
    "    metadata_path=str(dataset_path / \"train/all_annotations.csv\"),\n",
    "    preprocessors={\n",
    "        'spectrogram': {\n",
    "            '_target_': 'torchaudio.transforms.Spectrogram',\n",
    "            'n_fft': N_FFT,\n",
    "            'hop_length': HOP_LENGTH\n",
    "        },\n",
    "        'amplitude_2_db': {\n",
    "            '_target_': 'torchaudio.transforms.AmplitudeToDB'\n",
    "        },\n",
    "        'peak_norm': {\n",
    "            '_target_': 'soundbay.data.PeakNormalize'\n",
    "        }\n",
    "    },\n",
    "    seq_length=SEQ_LENGTH,\n",
    "    data_sample_rate=DATA_SAMPLE_RATE,\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    mode=\"val\",\n",
    "    slice_flag=SLICE_FLAG,\n",
    "    margin_ratio=MARGIN_RATIO,\n",
    "    augmentations=None,  \n",
    "    augmentations_p=0.0, \n",
    "    label_type='single_label',  \n",
    "    path_hierarchy=1\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7b49367f",
   "metadata": {},
   "source": [
    "# Read the annotations file\n",
    "annotations_path = dataset_path / \"train/all_annotations.csv\"\n",
    "annotations_df = pd.read_csv(annotations_path)\n",
    "# Get unique combinations of dataset and label\n",
    "unique_combinations = annotations_df[['dataset', 'label']].drop_duplicates().sort_values('dataset')\n",
    "unique_combinations.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "37212437",
   "metadata": {},
   "source": [
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_audio(file_path, begin_sample, duraion, sr=SAMPLE_RATE): \n",
    "    y, sr =  librosa.load(file_path, sr=sr, offset=begin_sample, duration=duraion)\n",
    "    return y\n",
    "\n",
    "def plot_spectrogram(spectrogram, sr=SAMPLE_RATE, hop_length=HOP_LENGTH, title=\"Mel Spectrogram\", \n",
    "                     start_time=None, end_time=None, low_freq=None, high_freq=None, linewidth=2):\n",
    "    \"\"\"Plot a spectrogram using librosa's display functions\"\"\"\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    \n",
    "    # Display the spectrogram\n",
    "    img = librosa.display.specshow(spectrogram, \n",
    "                            sr=sr,\n",
    "                            hop_length=hop_length,\n",
    "                            x_axis='time',\n",
    "                            y_axis='hz',\n",
    "                            )\n",
    "    \n",
    "    # Add bounding box if all parameters are provided\n",
    "    if all(x is not None for x in [start_time, end_time, low_freq, high_freq]):\n",
    "        # Create rectangle patch\n",
    "        import matplotlib.patches as patches\n",
    "        rect = patches.Rectangle(\n",
    "            (start_time - linewidth, low_freq-linewidth),  # (x,y)\n",
    "            end_time - start_time + linewidth,   # width\n",
    "            high_freq + linewidth - low_freq,    # height\n",
    "            linewidth=2,\n",
    "            edgecolor='r',\n",
    "            facecolor='none'\n",
    "        )\n",
    "        plt.gca().add_patch(rect)\n",
    "    \n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "31da11d6",
   "metadata": {},
   "source": [
    "import torch\n",
    "metadata = dataset.metadata.copy()\n",
    "# Plot spectrograms for each combination\n",
    "for _, row in unique_combinations.iloc[:20].iterrows():\n",
    "    dataset_name = row['dataset']\n",
    "    label = row['label']\n",
    "    \n",
    "    # Filter annotations for this combination\n",
    "    filtered_annotations = metadata[\n",
    "        (metadata['dataset'] == dataset_name) & \n",
    "        (metadata['label'].astype(str) == str(label))\n",
    "    ]\n",
    "    try: \n",
    "        filtered_annotations = filtered_annotations.sample(1)\n",
    "    \n",
    "        print(f\"\\nPlotting spectrograms for dataset: {dataset_name}, label: {label}\")\n",
    "        \n",
    "        # Plot first 3 samples for this combination\n",
    "        for i in filtered_annotations.index:\n",
    "            audio_processed, label, audio_raw, _ = dataset.__getitem__(i)\n",
    "            audio_processed = audio_processed.squeeze(0).detach().numpy()\n",
    "\n",
    "            # Get the filename and time range\n",
    "            data = metadata.loc[i]\n",
    "            filename = data['filename']\n",
    "            begin_time = data['begin_time']\n",
    "            end_time = data['end_time']\n",
    "            min_freq = data['low_frequency']\n",
    "            max_freq = data['high_frequency']\n",
    "            # Print some information about the sample\n",
    "            \n",
    "            try:\n",
    "                # Plot the spectrogram\n",
    "                plot_spectrogram(audio_processed, \n",
    "                            sr=SAMPLE_RATE,\n",
    "                            hop_length=HOP_LENGTH,\n",
    "                            title=f'Dataset: {dataset_name}, Sample {i}\\nLabel: {label}\\nTime: {begin_time:.2f}-{end_time:.2f}s, Duration: {end_time-begin_time:.2f}, min freq: {min_freq}, max freq: {max_freq}',\n",
    "                            start_time=0,\n",
    "                            end_time=end_time - begin_time,\n",
    "                            low_freq=min_freq,\n",
    "                            high_freq=max_freq\n",
    "                            )\n",
    "                \n",
    "                plt.show()\n",
    "                \n",
    "\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing sample {i}: {str(e)}\")\n",
    "    except: \n",
    "        pass"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pywhale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
