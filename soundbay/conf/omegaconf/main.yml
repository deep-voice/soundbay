main:
  data:
    batch_size: 64
    num_workers: 10
    data_sample_rate: 44100
    max_freq: 8000
    n_fft: 1024
    hop_length: 256
  optim:
    epochs: 100
    optimizer: 
      type: torch.optim.Adam
      lr: 0.001
    scheduler:
      type: torch.optim.lr_scheduler.ExponentialLR
      gamma: 0.995
    freeze_layers_for_finetune: False # Default is False.


  # augmentations: _augmentations
  # - preprocessors: _preprocessors
  # - model: defaults
  # - optim: defaults
  # - experiment: defaults