# @package _global_
optim:
  epochs: 2
  optimizer:
    _target_: torch.optim.SGD
    lr: 0.01
    momentum: 0.9
    weight_decay: 0.0001
  scheduler:
    _target_: torch.optim.lr_scheduler.ExponentialLR
    gamma: 0.926118
  freeze_layers_for_finetune: False # Default is False.