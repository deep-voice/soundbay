# @package _global_
optim:
  epochs: 100
  optimizer:
    _target_: torch.optim.Adam
    lr: 5e-4
  scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
    T_0: 5
  freeze_layers_for_finetune: True # Default is True.