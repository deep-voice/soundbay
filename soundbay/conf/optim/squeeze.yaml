# @package _global_
optim:
  epochs: 100
  optimizer:
    _target_: torch.optim.SGD
    lr: 0.001
    weight_decay: 0.0002
    momentum: 0.9
  scheduler:
    _target_: torch.optim.lr_scheduler.LinearLR
    start_factor: 1.0
    end_factor: 0
    total_iters: ${optim.epochs}
  freeze_layers_for_finetune:  # Default is False.